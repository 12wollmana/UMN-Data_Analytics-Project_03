{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import library as lib\n",
    "import seaborn as sns\n",
    "import numpy as npfrom scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_filepath = \"../images/plots/attributes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#import data without duplicates and without scaled\n",
    "df = lib.import_music_df_with_model(with_scaling = False, remove_dups = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Cluster'].value_counts().plot(kind='bar')\n",
    "# we can see cluste 0 is the majority followed by cluster 1 and cluster 2\n",
    "df['Cluster'].value_counts().plot(kind='bar').figure.savefig(f'{image_filepath}/total_songs_cluster.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = lib.import_music_df_with_model()\n",
    "df.head() # the correlation coefficient between each feature\n",
    "attr_corr= df.iloc[:,6:].corr()\n",
    "attr_corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the general correlation all the features share\n",
    "# the ones have weak or moderate correlation\n",
    "r=[]\n",
    "c=[]\n",
    "s=[]\n",
    "v=[]\n",
    "for i in range(attr_corr.shape[0]):\n",
    "    for j in range(attr_corr.shape[1]):\n",
    "        r.append(attr_corr.index[i])\n",
    "        c.append(attr_corr.columns[j])\n",
    "        v.append(abs(attr_corr.iloc[i,j]))\n",
    "        if attr_corr.iloc[i,j]!=1 and abs(attr_corr.iloc[i,j])>0.3 and abs(attr_corr.iloc[i,j])<0.5:\n",
    "            s.append('weak')\n",
    "        elif attr_corr.iloc[i,j]!=1 and abs(attr_corr.iloc[i,j])>0.5and abs(attr_corr.iloc[i,j])<0.7:\n",
    "            s.append('moderate')\n",
    "        elif attr_corr.iloc[i,j]!=1 and abs(attr_corr.iloc[i,j])>0.7:\n",
    "            s.append('strong')\n",
    "\n",
    "del r[5]\n",
    "del c[5]\n",
    "del s[5]\n",
    "del v[5]\n",
    "r=r[:-5]\n",
    "c=c[:-5]\n",
    "s=s[:-5]\n",
    "v=v[:-5]\n",
    "\n",
    "cor_df = pd.DataFrame({'atr1':r,'atr2':c,'strength':s,'value':v}).\\\n",
    "    sort_values('value',ascending=False).reset_index(drop=True)\n",
    "cor_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visuallize the correlation for all features\n",
    "import numpy as np\n",
    "# modified by https://seaborn.pydata.org/examples/many_pairwise_correlations.html\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(attr_corr, dtype=bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(11, 9))\n",
    "\n",
    "# Generate a custom diverging colormap\n",
    "cmap = sns.diverging_palette(230, 20, as_cmap=True)\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(attr_corr, mask=mask, cmap=cmap, vmin=-.5,vmax=.5, center=0,\n",
    "            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# create columns to store the labels,which is done by deviding the values for each attribute into 5 groups\n",
    "df_label = df.iloc[:,6:]\n",
    "df_label['count'] = 1\n",
    "for i in (df_label.columns[:-2]):\n",
    "    df_label[f'{i}_label'] = list(pd.cut(df_label[i],bins=5))\n",
    "df_label = df_label.iloc[:,9:]\n",
    "df_label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#calculate the independance between each features and cluster with Chi Square Test\n",
    "attribute_list = []\n",
    "sig_list = []\n",
    "p_value_list = []\n",
    "\n",
    "\n",
    "for i in (df.columns[6:-1]):\n",
    "    attri_raw = df_label[[f'{i}_label','count','Cluster']]\n",
    "\n",
    "        \n",
    "    cumulated_feature_by_cluster = attri_raw.groupby([f'{i}_label','Cluster'])['count'].sum()\n",
    "\n",
    "\n",
    "    chi_df =  cumulated_feature_by_cluster.unstack().fillna(0)\n",
    "    chi_df_np = chi_df.to_numpy()\n",
    "    \n",
    "    \n",
    "    attribute_list.append(i)\n",
    "    \n",
    "    chi_squared_stat, p_value, df_chi, expected_crosstab = stats.chi2_contingency(chi_df_np)\n",
    "    if p_value < 0.05:\n",
    "        sig_list.append('O')\n",
    "    else:\n",
    "        sig_list.append(' ')\n",
    "    p_value_list.append(p_value)\n",
    "\n",
    "sig_df = pd.DataFrame({'attribute':attribute_list,'significant':sig_list,'p_value':p_value_list})\n",
    "sig_df = sig_df.sort_values('p_value').reset_index(drop=True)\n",
    "sig_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(df[\"Cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dist_plot_cluster_percent(label_list=df.columns[6:-1]):\n",
    "label_list=sig_df['attribute'][:-1]\n",
    "def dist_plot_cluster_percent(label_list=label_list):\n",
    "    for i in label_list:\n",
    "        fig, axes = plt.subplots(1,3,sharex=True,sharey=True,figsize=(15,5))\n",
    "        plt.subplots_adjust(wspace = 0.5,hspace=0.5)\n",
    "        fig.suptitle(f'Distribution Plot for [{i}]',y=1.05)\n",
    "        attri_raw=df_label[[f'{i}_label','count','Cluster']]\n",
    "        x_axis=[]\n",
    "        for l in list(attri_raw[f'{i}_label'].sort_values().unique()):\n",
    "            x_axis.append(str(l))        \n",
    "       \n",
    "        if i == 'tempo':\n",
    "            for c in list(range(3)):\n",
    "                pd_raw=attri_raw[attri_raw['Cluster']==c]\n",
    "                pd_series=pd_raw[f'{i}_label']\n",
    "                mode=pd_series.value_counts().index[0]\n",
    "                color=['lightblue', 'lightblue', 'lightblue', 'lightblue', 'lightblue']\n",
    "                color[x_axis.index(str(mode))]='darkred'\n",
    "                y_axises={0:[0,0,31651,989,0],1:[11,14601,11682,0,0],2:[0,0,0,10160,811]}\n",
    "                y_axis = [i/sum(y_axises[c])*100 for i in y_axises[c]]\n",
    "                axes[c].bar(x=x_axis,height=y_axis,color=color)\n",
    "                axes[c].set_title(f'The mode of {i} for \\n[Cluster {c}] is {mode}')\n",
    "                axes[c].set_xticklabels(x_axis, rotation=90)\n",
    "                axes[0].set_ylabel('Percentage')\n",
    "            plt.savefig(f'{image_filepath}/{i}'.lower())\n",
    "                \n",
    "        else:\n",
    "            for c in list(range(3)):\n",
    "                pd_raw=attri_raw[attri_raw['Cluster']==c]\n",
    "                pd_series=pd_raw[f'{i}_label']\n",
    "                mode=pd_series.value_counts().index[0]\n",
    "                color=['lightblue', 'lightblue', 'lightblue', 'lightblue', 'lightblue']\n",
    "                color[x_axis.index(str(mode))]='darkred'\n",
    "                y_axis=pd_raw.groupby(f'{i}_label')['count'].sum()\n",
    "                total=sum(y_axis)\n",
    "                y_axis=[i/total*100 for i in y_axis]\n",
    "                axes[c].bar(x=x_axis,height=y_axis,color=color)\n",
    "                axes[c].set_title(f'The mode of {i} for \\n[Cluster {c}] is {mode}')\n",
    "                axes[c].set_xticklabels(x_axis, rotation=90)\n",
    "                axes[0].set_ylabel('Percentage')\n",
    "            plt.savefig(f'{image_filepath}/{i}_percent'.lower())\n",
    "        \n",
    "        plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dist_plot_cluster_percent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "clf = RandomForestClassifier(max_depth=3,n_estimators=100)\n",
    "clf.fit(X_train, y_train)\n",
    "clf.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = clf.predict(X_test)\n",
    "errors = abs(predictions - y_test)\n",
    "total_error_counts=[1 if i!=0 else 0 for i in errors]\n",
    "# Print out the mean absolute error (mae)\n",
    "print('Mean Absolute Error:', round(np.mean(errors), 2), 'degrees.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate mean absolute percentage error (MAPE)\n",
    "mape = 100 * (sum(total_error_counts) / len(y_test))\n",
    "# Calculate and display accuracy\n",
    "accuracy = 100 - np.mean(mape)\n",
    "print('Accuracy:', round(accuracy, 2), '%.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame({'feature':feature_cols.columns,'importance':clf.feature_importances_}).sort_values('importance',ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cluster has better corr with these\n",
    "cluster=[['Cluster','danceability'],['Cluster','speechiness'],['Cluster','tempo']]\n",
    "\n",
    "#t test?\n",
    "#chi square test?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
